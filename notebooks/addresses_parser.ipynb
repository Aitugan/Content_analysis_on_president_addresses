{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install matplotlib.pyplot\n",
    "!pip install dateparser\n",
    "!pip install --user -U nltk\n",
    "!pip install pystemmer\n",
    "!pip install langdetect\n",
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aitugan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "import dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_addresses_links():\n",
    "  url = 'https://www.akorda.kz/ru/addresses' #should be 27 addresses\n",
    "  pages_num = 3\n",
    "  addresses_links = {}\n",
    "  for page in range(1, pages_num+1):\n",
    "    response = requests.get(url+\"?page=\"+str(page))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "      card_divs = soup.find_all(\"div\", \"card\")\n",
    "\n",
    "      for card in card_divs:\n",
    "        view_div = card.find(\"div\", \"view\")\n",
    "        a_tag = view_div.find('a')\n",
    "\n",
    "        mt3_div = card.find(\"div\", \"mt-3\")\n",
    "        h5_tag = mt3_div.find(\"h5\", \"mt-3\")\n",
    "\n",
    "        if a_tag:\n",
    "          link = \"https://www.akorda.kz\" + a_tag['href']\n",
    "          addresses_links[h5_tag.get_text()] = link\n",
    "\n",
    "    else:\n",
    "      print(f\"Failed to scrape the website. Status code: {response.status_code}\")\n",
    "\n",
    "  return addresses_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  characters_to_remove = ['«', '»', '“', '”', '•', '\\xa0', '\\r','\\t', '…', '–','—','№','0','1','2','3','4','5','6','7','8','9','„','‟']\n",
    "  pattern = '[' + re.escape(''.join(characters_to_remove)) + ']'\n",
    "  text = re.sub(pattern, ' ', text)\n",
    "  return text\n",
    "\n",
    "def tokenize_words(text):\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "  text = text.lower()\n",
    "  text = [word.strip() for word in text.split(\" \") if word and word.strip()]\n",
    "\n",
    "  return text\n",
    "\n",
    "def format_date(date_string):\n",
    "    date_object = dateparser.parse(date_string, languages=['ru'])\n",
    "    formatted_date = date_object.strftime(\"%d/%m/%Y\")\n",
    "    return formatted_date\n",
    "\n",
    "def contains_kazakh_letters(text):\n",
    "    kazakh_letters_pattern = r'[әіңғүұқөһӘІҢҒҮҰҚӨҺ]'\n",
    "    matches = re.findall(kazakh_letters_pattern, text, re.IGNORECASE)\n",
    "    return bool(matches)\n",
    "\n",
    "def parse_addresses(addresses_links, isCleaned=False, isTokenized=False):\n",
    "  pages = {}\n",
    "  for date, link in addresses_links.items():\n",
    "    response = requests.get(link)\n",
    "\n",
    "    date=format_date(date)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      outer_div = soup.find(\"div\", \"mt-5\")\n",
    "      article_div = outer_div.find(\"article\")\n",
    "\n",
    "      p_tags = article_div.find_all('p')\n",
    "\n",
    "      for p in p_tags:\n",
    "        text = p.get_text()\n",
    "        if contains_kazakh_letters(text):\n",
    "          continue\n",
    "        if isCleaned: text = clean_text(text)\n",
    "        if isTokenized: text = tokenize_words(text)\n",
    "\n",
    "        if len(text) == 0: continue\n",
    "\n",
    "        if date in pages:\n",
    "          pages[date] += text+\" \"\n",
    "        else:\n",
    "          pages[date] = text\n",
    "\n",
    "    else:\n",
    "      print(f\"Failed to scrape the website. Status code: {response.status_code}\")\n",
    "  return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "def save_data_to_json(title, data):\n",
    "  directory = f'../addresses'\n",
    "  os.makedirs(directory, exist_ok=True)\n",
    "  title = title.replace('\"', '')\n",
    "  title = title.replace('/', '_')\n",
    "  file_path = os.path.join(directory, f'{title}.json')\n",
    "  with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "      json.dump(data, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_links = parse_addresses_links()\n",
    "pages = parse_addresses(addresses_links, True)\n",
    "save_data_to_json(\"addresses\", pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

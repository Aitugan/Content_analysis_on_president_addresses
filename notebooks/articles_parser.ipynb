{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1655a29-ed2c-4cf9-9102-eea1d920d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (from bs4) (4.8.2)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=962a45bea1bfa6d6228df0d2b376f0b532460283b00c3e17c8360965b6803a1d\n",
      "  Stored in directory: /home/aitugan/.cache/pip/wheels/75/78/21/68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0df6424-5e54-4b19-88a5-1cf703689c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d73e81b-89dd-4f58-bb4e-38f8acfcd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = {\n",
    "  \"economic\": \"Экономика\",\n",
    "  \"culture\": \"Культура\",\n",
    "  \"medicine\": \"Медицина\",\n",
    "  \"science\": \"Наука\",\n",
    "  \"tech\": \"Технологии\",\n",
    "  \"tengri-sport\": \"Спорт\",\n",
    "  \"life\":\"Жизнь\",\n",
    "  \"show\": \"Шоу-бизнес\",\n",
    "  \"accidents\":\"Происшествия\",\n",
    "  \"crime\":\"Преступность\",\n",
    "}\n",
    "URL = \"https://tengrinews.kz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0233e076-b8e8-4e9f-acec-659bf64b6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_links(url, topic, pages_num=50):\n",
    "  path = '{}/{}'.format(url,topic)\n",
    "  articles_links = {}\n",
    "  for page in range(1, pages_num+1):\n",
    "    response = requests.get(\"{}/page/{}\".format(path, str(page)))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "      article_divs = soup.find_all(\"div\", \"tn-article-item\")\n",
    "\n",
    "      for article in article_divs:\n",
    "        a_tag = article.find('a')\n",
    "\n",
    "        title = article.find(\"span\", \"tn-article-title\")\n",
    "        if title:\n",
    "          title = title.get_text()\n",
    "\n",
    "        if a_tag:\n",
    "          link = url + a_tag['href']\n",
    "          articles_links[title] = link\n",
    "\n",
    "    else:\n",
    "      print(f\"Failed to scrape the website. Status code: {response.status_code}\")\n",
    "  return articles_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e692c79-0274-401f-b96d-209968723f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_article_to_file(title, article, topic):\n",
    "  directory = f'../articles/{topic}'\n",
    "  os.makedirs(directory, exist_ok=True)\n",
    "  title = title.replace('\"', '')\n",
    "  title = title.replace('/', '_')\n",
    "  file_path = os.path.join(directory, f'{title}.txt')\n",
    "  with open(file_path, 'w') as file:\n",
    "      file.write(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc13169-8466-459c-a1f2-2bd7e4f770de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles_by_topic(topic):\n",
    "  links = parse_links(URL, topic)\n",
    "  for title, link in links.items():\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      article_content = soup.find(\"div\", \"tn-news-content\")\n",
    "      if article_content is None:\n",
    "        print(link)\n",
    "        continue\n",
    "      p_tags = article_content.find_all(\"p\")\n",
    "      article = []\n",
    "      for i in range(len(p_tags)-2):\n",
    "        article.append(p_tags[i].get_text())\n",
    "      save_article_to_file(title, \"\".join(article), topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9075b0ea-2688-417f-8f21-a0d0a71859b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in TOPICS:\n",
    "  if topic == \"tengri-sport\": continue\n",
    "  parse_articles_by_topic(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c98b6-58d7-4d9a-98cc-715b2b1c095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_files_in_folder(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473fad9f-047e-4323-aa44-eb56b2383b92",
   "metadata": {},
   "source": [
    "# Parser for sport articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7072bd19-9638-4aa6-b9fe-f6134eb9bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sport_articles(topic):\n",
    "  links = parse_links(URL, topic)\n",
    "  for title, link in links.items():\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      article_content = soup.find(\"div\", \"news__text\")\n",
    "      if article_content is None:\n",
    "        print(link)\n",
    "        continue\n",
    "      p_tags = article_content.find_all(\"p\")\n",
    "      article = []\n",
    "      for i in range(len(p_tags)-1):\n",
    "        article.append(p_tags[i].get_text())\n",
    "      save_article_to_file(title, \"\".join(article), topic)\n",
    "\n",
    "parse_sport_articles(\"tengri-sport\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

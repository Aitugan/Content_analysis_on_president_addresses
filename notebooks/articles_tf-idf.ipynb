{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1655a29-ed2c-4cf9-9102-eea1d920d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (from bs4) (4.8.2)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=962a45bea1bfa6d6228df0d2b376f0b532460283b00c3e17c8360965b6803a1d\n",
      "  Stored in directory: /home/aitugan/.cache/pip/wheels/75/78/21/68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0df6424-5e54-4b19-88a5-1cf703689c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "import math\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d73e81b-89dd-4f58-bb4e-38f8acfcd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = {\n",
    "  \"economic\": \"Экономика\",\n",
    "  \"culture\": \"Культура\",\n",
    "  \"medicine\": \"Медицина\",\n",
    "  \"science\": \"Наука\",\n",
    "  \"tech\": \"Технологии\",\n",
    "  \"tengri-sport\": \"Спорт\",\n",
    "  \"life\":\"Жизнь\",\n",
    "  \"show\": \"Шоу-бизнес\",\n",
    "  \"accidents\":\"Происшествия\",\n",
    "  \"crime\":\"Преступность\",\n",
    "}\n",
    "URL = \"https://tengrinews.kz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0233e076-b8e8-4e9f-acec-659bf64b6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_links(url, topic, pages_num=50):\n",
    "  path = '{}/{}'.format(url,topic)\n",
    "  articles_links = {}\n",
    "  for page in range(1, pages_num+1):\n",
    "    response = requests.get(\"{}/page/{}\".format(path, str(page)))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "      article_divs = soup.find_all(\"div\", \"tn-article-item\")\n",
    "\n",
    "      for article in article_divs:\n",
    "        a_tag = article.find('a')\n",
    "\n",
    "        title = article.find(\"span\", \"tn-article-title\")\n",
    "        if title:\n",
    "          title = title.get_text()\n",
    "\n",
    "        if a_tag:\n",
    "          link = url + a_tag['href']\n",
    "          articles_links[title] = link\n",
    "\n",
    "    else:\n",
    "      print(f\"Failed to scrape the website. Status code: {response.status_code}\")\n",
    "  return articles_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e692c79-0274-401f-b96d-209968723f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_article_to_file(title, article, topic):\n",
    "  directory = f'../main/{topic}'\n",
    "  os.makedirs(directory, exist_ok=True)\n",
    "  title = title.replace('\"', '')\n",
    "  title = title.replace('/', '_')\n",
    "  file_path = os.path.join(directory, f'{title}.txt')\n",
    "  with open(file_path, 'w') as file:\n",
    "      file.write(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc13169-8466-459c-a1f2-2bd7e4f770de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_articles_by_topic(topic):\n",
    "  links = parse_links(URL, topic)\n",
    "  for title, link in links.items():\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      article_content = soup.find(\"div\", \"tn-news-content\")\n",
    "      if article_content is None:\n",
    "        print(link)\n",
    "        continue\n",
    "      p_tags = article_content.find_all(\"p\")\n",
    "      article = []\n",
    "      for i in range(len(p_tags)-2):\n",
    "        article.append(p_tags[i].get_text())\n",
    "      save_article_to_file(title, \"\".join(article), topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9075b0ea-2688-417f-8f21-a0d0a71859b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in TOPICS:\n",
    "  if topic == \"tengri-sport\": continue\n",
    "  parse_articles_by_topic(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c98b6-58d7-4d9a-98cc-715b2b1c095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def delete_files_in_folder(folder_path):\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "\n",
    "delete_files_in_folder(folder_to_clear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473fad9f-047e-4323-aa44-eb56b2383b92",
   "metadata": {},
   "source": [
    "# Parser for sport articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7072bd19-9638-4aa6-b9fe-f6134eb9bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sport_articles(topic):\n",
    "  links = parse_links(URL, topic)\n",
    "  for title, link in links.items():\n",
    "    response = requests.get(link)\n",
    "    if response.status_code == 200:\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      article_content = soup.find(\"div\", \"news__text\")\n",
    "      if article_content is None:\n",
    "        print(link)\n",
    "        continue\n",
    "      p_tags = article_content.find_all(\"p\")\n",
    "      article = []\n",
    "      for i in range(len(p_tags)-1):\n",
    "        article.append(p_tags[i].get_text())\n",
    "      save_article_to_file(title, \"\".join(article), topic)\n",
    "\n",
    "parse_sport_articles(\"tengri-sport\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de5b62b-3c6e-4f81-bbaa-4e7c2af85538",
   "metadata": {},
   "source": [
    "# TF-IDF for articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c6c8b-cec7-464d-9f3b-7e0aaa3c57b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install dateparser\n",
    "!pip install --user -U nltk\n",
    "!pip install pystemmer\n",
    "!pip install langdetect\n",
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb9a66e-44a6-40cf-a6e0-39c2239394f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aitugan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "import dateparser\n",
    "from datetime import datetime\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymorphy2\n",
    "import Stemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24396b8b-892c-4ac7-b98d-908d9efc8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  characters_to_remove = ['«', '»', '“', '”', '•', '\\xa0', '\\r','\\t', '…', '–','—','№','0','1','2','3','4','5','6','7','8','9','„','‟']\n",
    "  pattern = '[' + re.escape(''.join(characters_to_remove)) + ']'\n",
    "  text = re.sub(pattern, ' ', text)\n",
    "  return text\n",
    "\n",
    "def tokenize_words(text):\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "  text = text.lower()\n",
    "  text = [word.strip() for word in text.split(\" \") if word and word.strip()]\n",
    "\n",
    "  return text\n",
    "\n",
    "def contains_kazakh_letters(text):\n",
    "    kazakh_letters_pattern = r'[әіңғүұқөһӘІҢҒҮҰҚӨҺ]'\n",
    "    matches = re.findall(kazakh_letters_pattern, text, re.IGNORECASE)\n",
    "    return bool(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd32d3a0-2c90-4626-8543-355805a7b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(topic_pages):\n",
    "    sent_tokenized_page = {}\n",
    "    for topic, pages in topic_pages.items():\n",
    "      tokenized_article = []\n",
    "      for page in pages:\n",
    "        if contains_kazakh_letters(page):\n",
    "          continue\n",
    "\n",
    "        sentence_tokens = sent_tokenize(clean_text(page))\n",
    "        for i, sentence in enumerate(sentence_tokens):\n",
    "          sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "          sentence = sentence.lower()\n",
    "        tokenized_article.append(sentence_tokens)\n",
    "\n",
    "      if topic in sent_tokenized_page:\n",
    "        sent_tokenized_page[topic].append(tokenized_article)\n",
    "      else:\n",
    "        sent_tokenized_page[topic] = tokenized_article\n",
    "    # print(sent_tokenized_page)\n",
    "    return sent_tokenized_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c9763fd-0080-4928-ba20-a1d19b388339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(sent_tokenized_page):\n",
    "    word_tokenized_page = {}\n",
    "    for topic, pages in sent_tokenized_page.items():\n",
    "      for page in pages:\n",
    "        page_data = []\n",
    "        for sentence in page:\n",
    "          word_tokens = word_tokenize(sentence)\n",
    "          page_data.append(word_tokens)\n",
    "        if topic in word_tokenized_page:\n",
    "          word_tokenized_page[topic].append(page_data)\n",
    "        else:\n",
    "          word_tokenized_page[topic] = [page_data]\n",
    "    # print(word_tokenized_page)\n",
    "    return word_tokenized_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb516a1-2b00-45ab-8233-09b3d2ce8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_words(topic_articles_dict):\n",
    "    sent_tokenized_page = tokenize_sentences(topic_articles_dict)\n",
    "    word_tokenized_page = tokenize_words(sent_tokenized_page)\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    # morphed_words = {}\n",
    "    morphed_words_no_ending = {}\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    for topic, pages in word_tokenized_page.items():\n",
    "      for page in pages:\n",
    "        page_data = []\n",
    "        for sentence in page:\n",
    "          for i in range(len(sentence)):\n",
    "            first_tag = morph.parse(sentence[i].lower())[0]\n",
    "            second_tag = morph.parse(sentence[i-1].lower())[0]\n",
    "            if first_tag.tag.POS == 'NOUN' and (second_tag.tag.POS == 'ADJF' or second_tag.tag.POS == 'ADJS'):\n",
    "      \n",
    "              cut_adj = stemmer.stemWord(sentence[i-1].lower())\n",
    "              cut_noun = stemmer.stemWord(sentence[i].lower())\n",
    "      \n",
    "              normal_noun_tag = morph.parse(first_tag.normal_form)[0].tag\n",
    "              normal_noun = first_tag.normal_form\n",
    "              normal_adj = morph.parse(second_tag.normal_form)[0]\n",
    "      \n",
    "              if normal_noun_tag.gender is not None:\n",
    "                if normal_noun_tag.gender == 'masc' and normal_adj.inflect({'masc'}):\n",
    "                  normal_adj = normal_adj.inflect({'masc'}).word\n",
    "                elif normal_noun_tag.gender == 'femn' and normal_adj.inflect({'femn'}):\n",
    "                  normal_adj = normal_adj.inflect({'femn'}).word\n",
    "                elif normal_noun_tag.gender == 'neut' and normal_adj.inflect({'neut'}):\n",
    "                  normal_adj = normal_adj.inflect({'neut'}).word\n",
    "                else:\n",
    "                  normal_adj = morph.parse(second_tag.normal_form)[0].word\n",
    "              else:\n",
    "                normal_adj = morph.parse(second_tag.normal_form)[0].word\n",
    "      \n",
    "              page_data.append([cut_adj+\" \"+cut_noun, normal_adj+\" \"+normal_noun])\n",
    "\n",
    "        if topic in morphed_words_no_ending:\n",
    "          morphed_words_no_ending[topic].append(page_data)\n",
    "        else:\n",
    "          morphed_words_no_ending[topic] = [page_data]\n",
    "    return morphed_words_no_ending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b7349b3-4080-4d15-93f8-22ec4c95e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_tf_by_roots(roots):\n",
    "  word_tf = defaultdict(lambda: defaultdict(list))\n",
    "  for topic, pages in roots.items():\n",
    "    for page in pages:\n",
    "      count = {}\n",
    "      for phrase in page:\n",
    "        if (phrase[0],phrase[1]) in count:\n",
    "          count[(phrase[0],phrase[1])] += 1\n",
    "        else:\n",
    "          count[(phrase[0],phrase[1])] = 1\n",
    "\n",
    "      total = len(page)\n",
    "      for phrase, cnt in count.items():\n",
    "        word_tf[topic][phrase[0]] = [count[phrase]/total, phrase[1]]\n",
    "\n",
    "  return word_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22321f1-e82e-4191-82a7-a98b22b2366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_idf_by_roots(roots):\n",
    "  word_cnt = defaultdict(int)\n",
    "  word_idf = defaultdict(list)\n",
    "\n",
    "  for topic, pages in roots.items():\n",
    "    for page in pages:\n",
    "      # for phrases in :\n",
    "        phrases_list = [(phrase[0],phrase[1]) for phrase in page]\n",
    "        phrases_set = set(phrases_list)\n",
    "        for phrase in phrases_set:\n",
    "          # print(phrase, \"phrases_set: \",phrases_set)\n",
    "          word_cnt[phrase] += 1\n",
    "\n",
    "    for phrase, doc_freq in word_cnt.items():\n",
    "      word_idf[phrase[0]] = [math.log(len(roots[topic]) / word_cnt[phrase]), phrase[1]]\n",
    "\n",
    "  return word_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd99039-4129-47d5-8f11-8b6ccc488f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_idf_by_roots(roots, words_tf, words_idf):\n",
    "  word_tf_idf_by_topic={}\n",
    "  # print(words_tf.items())\n",
    "  # print(words_idf)\n",
    "\n",
    "  for topic, pages in roots.items():\n",
    "    word_tf_idf = {}\n",
    "    for page in pages:\n",
    "      # print(words_tf[topic])\n",
    "      for phrase in page:\n",
    "        # if phrase == 'уязвим категор' or phrase == 'синтетическ наркотик' or phrase == 'электоральн цикл':\n",
    "        #   print(phrase, words_tf[topic][phrase], words_idf[phrase])\n",
    "        if phrase[0] in words_tf[topic] and phrase[0] in words_idf:\n",
    "          # print(\"words_tf[topic][phrase[0]][0]\",words_tf[topic][phrase[0]][0])\n",
    "          # print(\"words_idf[phrase[0]][0]\",words_idf[phrase[0]][0])\n",
    "          word_tf_idf[(phrase[0],phrase[1])] = words_tf[topic][phrase[0]][0] * words_idf[phrase[0]][0]\n",
    "        else:\n",
    "          # print(phrase, \"in word_tf: \", phrase[0] in words_tf)\n",
    "          # print(phrase, \"in word_idf: \", phrase[0] in words_idf)\n",
    "          break\n",
    "      # sorted_word_tf_idf_in_doc = sorted([(item[0][1],item[1]) for item in word_tf_idf.items()], key=lambda item: item[1], reverse=True)\n",
    "      \n",
    "      word_tf_idf_by_topic[topic] = sorted([(item[0][0],item[1]) for item in word_tf_idf.items()], key=lambda item: item[1], reverse=True)#sorted_word_tf_idf_in_doc[:5]\n",
    "  return word_tf_idf_by_topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3a44e-11ec-4c45-82f4-2e2ecfd24fb4",
   "metadata": {},
   "source": [
    "### Iterate over files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9b2e065-9015-4efd-9253-61c47e84dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "def extract_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "json_file_path = '../main/data2.json'\n",
    "\n",
    "if os.path.exists(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        topic_tf_idf = json.load(json_file)\n",
    "else:\n",
    "    topic_tf_idf = {}\n",
    "\n",
    "for topic in TOPICS:\n",
    "    topic_articles = {}\n",
    "    folder_path = f'../main/{topic}'\n",
    "    # Iterate over files in the folder\n",
    "    articles = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
    "            text = extract_text_from_file(file_path)\n",
    "            articles.append(text)\n",
    "            # print(f\"Text from '{filename}':\\n{text}\\n{'='*50}\")\n",
    "\n",
    "    pages = len(articles)\n",
    "    topic_articles[topic] = articles\n",
    "    morphed_words = morph_words(topic_articles)\n",
    "    word_tf = count_word_tf_by_roots(morphed_words)\n",
    "    word_idf = count_word_idf_by_roots(morphed_words)\n",
    "    word_tf_idf = calculate_tf_idf_by_roots(morphed_words, word_tf, word_idf)\n",
    "    topic_tf_idf[topic] = word_tf_idf[topic]\n",
    "\n",
    "with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(topic_tf_idf, json_file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8fb233",
   "metadata": {},
   "source": [
    "### Bring data to map format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e7e5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../main/data2.json\"\n",
    "data = {}\n",
    "if os.path.exists(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9a5917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map_format = {}\n",
    "for topic, word_data in data.items():\n",
    "    for word_score in word_data:\n",
    "        word, score = word_score\n",
    "        if word not in data_map_format:\n",
    "            data_map_format[word] = [score, topic]\n",
    "        else:\n",
    "            prev_score = data_map_format[word][0]\n",
    "            if prev_score < score:\n",
    "                data_map_format[word] = [score, topic]\n",
    "\n",
    "json_file_path = \"../main/map_data.json\"\n",
    "with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data_map_format, json_file, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

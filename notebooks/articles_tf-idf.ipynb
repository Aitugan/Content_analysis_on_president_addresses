{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d73e81b-89dd-4f58-bb4e-38f8acfcd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = {\n",
    "  \"economic\": \"Экономика\",\n",
    "  \"culture\": \"Культура\",\n",
    "  \"medicine\": \"Медицина\",\n",
    "  \"science\": \"Наука\",\n",
    "  \"tech\": \"Технологии\",\n",
    "  \"tengri-sport\": \"Спорт\",\n",
    "  \"life\":\"Жизнь\",\n",
    "  \"show\": \"Шоу-бизнес\",\n",
    "  \"accidents\":\"Происшествия\",\n",
    "  \"crime\":\"Преступность\",\n",
    "}\n",
    "URL = \"https://tengrinews.kz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de5b62b-3c6e-4f81-bbaa-4e7c2af85538",
   "metadata": {},
   "source": [
    "# TF-IDF for articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c6c8b-cec7-464d-9f3b-7e0aaa3c57b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install dateparser\n",
    "!pip install --user -U nltk\n",
    "!pip install pystemmer\n",
    "!pip install langdetect\n",
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb9a66e-44a6-40cf-a6e0-39c2239394f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aitugan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymorphy2\n",
    "import Stemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24396b8b-892c-4ac7-b98d-908d9efc8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  characters_to_remove = ['«', '»', '“', '”', '•', '\\xa0', '\\r','\\t', '…', '–','—','№','0','1','2','3','4','5','6','7','8','9','„','‟']\n",
    "  pattern = '[' + re.escape(''.join(characters_to_remove)) + ']'\n",
    "  text = re.sub(pattern, ' ', text)\n",
    "  return text\n",
    "\n",
    "def tokenize_words(text):\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "  text = text.lower()\n",
    "  text = [word.strip() for word in text.split(\" \") if word and word.strip()]\n",
    "\n",
    "  return text\n",
    "\n",
    "def contains_kazakh_letters(text):\n",
    "    kazakh_letters_pattern = r'[әіңғүұқөһӘІҢҒҮҰҚӨҺ]'\n",
    "    matches = re.findall(kazakh_letters_pattern, text, re.IGNORECASE)\n",
    "    return bool(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd32d3a0-2c90-4626-8543-355805a7b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(topic_pages):\n",
    "    sent_tokenized_page = {}\n",
    "    for topic, pages in topic_pages.items():\n",
    "      tokenized_article = []\n",
    "      for page in pages:\n",
    "        if contains_kazakh_letters(page):\n",
    "          continue\n",
    "\n",
    "        sentence_tokens = sent_tokenize(clean_text(page))\n",
    "        for i, sentence in enumerate(sentence_tokens):\n",
    "          sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "          sentence = sentence.lower()\n",
    "        tokenized_article.append(sentence_tokens)\n",
    "\n",
    "      if topic in sent_tokenized_page:\n",
    "        sent_tokenized_page[topic].append(tokenized_article)\n",
    "      else:\n",
    "        sent_tokenized_page[topic] = tokenized_article\n",
    "\n",
    "    return sent_tokenized_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c9763fd-0080-4928-ba20-a1d19b388339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(sent_tokenized_page):\n",
    "    word_tokenized_page = {}\n",
    "    for topic, pages in sent_tokenized_page.items():\n",
    "      for page in pages:\n",
    "        page_data = []\n",
    "        for sentence in page:\n",
    "          word_tokens = word_tokenize(sentence)\n",
    "          page_data.append(word_tokens)\n",
    "        if topic in word_tokenized_page:\n",
    "          word_tokenized_page[topic].append(page_data)\n",
    "        else:\n",
    "          word_tokenized_page[topic] = [page_data]\n",
    "\n",
    "    return word_tokenized_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb516a1-2b00-45ab-8233-09b3d2ce8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_words(topic_articles_dict):\n",
    "    sent_tokenized_page = tokenize_sentences(topic_articles_dict)\n",
    "    word_tokenized_page = tokenize_words(sent_tokenized_page)\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    morphed_words_no_ending = {}\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    for topic, pages in word_tokenized_page.items():\n",
    "      for page in pages:\n",
    "        page_data = []\n",
    "        for sentence in page:\n",
    "          for i in range(len(sentence)):\n",
    "            first_tag = morph.parse(sentence[i].lower())[0]\n",
    "            second_tag = morph.parse(sentence[i-1].lower())[0]\n",
    "            if first_tag.tag.POS == 'NOUN' and (second_tag.tag.POS == 'ADJF' or second_tag.tag.POS == 'ADJS'):\n",
    "      \n",
    "              cut_adj = stemmer.stemWord(sentence[i-1].lower())\n",
    "              cut_noun = stemmer.stemWord(sentence[i].lower())\n",
    "      \n",
    "              normal_noun_tag = morph.parse(first_tag.normal_form)[0].tag\n",
    "              normal_noun = first_tag.normal_form\n",
    "              normal_adj = morph.parse(second_tag.normal_form)[0]\n",
    "      \n",
    "              if normal_noun_tag.gender is not None:\n",
    "                if normal_noun_tag.gender == 'masc' and normal_adj.inflect({'masc'}):\n",
    "                  normal_adj = normal_adj.inflect({'masc'}).word\n",
    "                elif normal_noun_tag.gender == 'femn' and normal_adj.inflect({'femn'}):\n",
    "                  normal_adj = normal_adj.inflect({'femn'}).word\n",
    "                elif normal_noun_tag.gender == 'neut' and normal_adj.inflect({'neut'}):\n",
    "                  normal_adj = normal_adj.inflect({'neut'}).word\n",
    "                else:\n",
    "                  normal_adj = morph.parse(second_tag.normal_form)[0].word\n",
    "              else:\n",
    "                normal_adj = morph.parse(second_tag.normal_form)[0].word\n",
    "      \n",
    "              page_data.append([cut_adj+\" \"+cut_noun, normal_adj+\" \"+normal_noun])\n",
    "\n",
    "        if topic in morphed_words_no_ending:\n",
    "          morphed_words_no_ending[topic].append(page_data)\n",
    "        else:\n",
    "          morphed_words_no_ending[topic] = [page_data]\n",
    "    return morphed_words_no_ending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b7349b3-4080-4d15-93f8-22ec4c95e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_tf_by_roots(roots):\n",
    "  word_tf = defaultdict(lambda: defaultdict(list))\n",
    "  for topic, pages in roots.items():\n",
    "    for page in pages:\n",
    "      count = {}\n",
    "      for phrase in page:\n",
    "        if (phrase[0],phrase[1]) in count:\n",
    "          count[(phrase[0],phrase[1])] += 1\n",
    "        else:\n",
    "          count[(phrase[0],phrase[1])] = 1\n",
    "\n",
    "      total = len(page)\n",
    "      for phrase, cnt in count.items():\n",
    "        word_tf[topic][phrase[0]] = [count[phrase]/total, phrase[1]]\n",
    "\n",
    "  return word_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22321f1-e82e-4191-82a7-a98b22b2366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_idf_by_roots(roots):\n",
    "  word_cnt = defaultdict(int)\n",
    "  word_idf = defaultdict(list)\n",
    "\n",
    "  for topic, pages in roots.items():\n",
    "    for page in pages:\n",
    "        phrases_list = [(phrase[0],phrase[1]) for phrase in page]\n",
    "        phrases_set = set(phrases_list)\n",
    "        for phrase in phrases_set:\n",
    "          word_cnt[phrase] += 1\n",
    "\n",
    "    for phrase, doc_freq in word_cnt.items():\n",
    "      word_idf[phrase[0]] = [math.log(len(roots[topic]) / word_cnt[phrase]), phrase[1]]\n",
    "\n",
    "  return word_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd99039-4129-47d5-8f11-8b6ccc488f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_idf_by_roots(roots, words_tf, words_idf):\n",
    "  word_tf_idf_by_topic={}\n",
    "\n",
    "  for topic, pages in roots.items():\n",
    "    word_tf_idf = {}\n",
    "    for page in pages:\n",
    "      for phrase in page:\n",
    "        if phrase[0] in words_tf[topic] and phrase[0] in words_idf:\n",
    "          word_tf_idf[(phrase[0],phrase[1])] = words_tf[topic][phrase[0]][0] * words_idf[phrase[0]][0]\n",
    "        else:\n",
    "          break\n",
    "      \n",
    "      word_tf_idf_by_topic[topic] = sorted([(item[0][0],item[1]) for item in word_tf_idf.items()], key=lambda item: item[1], reverse=True)#sorted_word_tf_idf_in_doc[:5]\n",
    "  return word_tf_idf_by_topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3a44e-11ec-4c45-82f4-2e2ecfd24fb4",
   "metadata": {},
   "source": [
    "### Iterate over files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9b2e065-9015-4efd-9253-61c47e84dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "def extract_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "json_file_path = '../articles/data.json'\n",
    "\n",
    "if os.path.exists(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        topic_tf_idf = json.load(json_file)\n",
    "else:\n",
    "    topic_tf_idf = {}\n",
    "\n",
    "for topic in TOPICS:\n",
    "    topic_articles = {}\n",
    "    folder_path = f'../articles/{topic}'\n",
    "    # Iterate over files in the folder\n",
    "    articles = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
    "            text = extract_text_from_file(file_path)\n",
    "            articles.append(text)\n",
    "\n",
    "    pages = len(articles)\n",
    "    topic_articles[topic] = articles\n",
    "    morphed_words = morph_words(topic_articles)\n",
    "    word_tf = count_word_tf_by_roots(morphed_words)\n",
    "    word_idf = count_word_idf_by_roots(morphed_words)\n",
    "    word_tf_idf = calculate_tf_idf_by_roots(morphed_words, word_tf, word_idf)\n",
    "    topic_tf_idf[topic] = word_tf_idf[topic]\n",
    "\n",
    "with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(topic_tf_idf, json_file, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8fb233",
   "metadata": {},
   "source": [
    "### Bring data to map format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e7e5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../articles/data.json\"\n",
    "data = {}\n",
    "if os.path.exists(json_file_path):\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9a5917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map_format = {}\n",
    "for topic, word_data in data.items():\n",
    "    for word_score in word_data:\n",
    "        word, score = word_score\n",
    "        if word not in data_map_format:\n",
    "            data_map_format[word] = [score, topic]\n",
    "        else:\n",
    "            prev_score = data_map_format[word][0]\n",
    "            if prev_score < score:\n",
    "                data_map_format[word] = [score, topic]\n",
    "\n",
    "json_file_path = \"../articles/map_data.json\"\n",
    "with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data_map_format, json_file, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

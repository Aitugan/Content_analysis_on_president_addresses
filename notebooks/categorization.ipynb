{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TF-IDF data from addresses and articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "def read_json_data(json_file_path):\n",
    "    data = {}\n",
    "    if os.path.exists(json_file_path):\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "def save_data_to_json(title, data):\n",
    "  directory = f'../addresses'\n",
    "  os.makedirs(directory, exist_ok=True)\n",
    "  title = title.replace('\"', '')\n",
    "  title = title.replace('/', '_')\n",
    "  file_path = os.path.join(directory, f'{title}.json')\n",
    "  with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "      json.dump(data, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aitugan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "import Stemmer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "\n",
    "def morph_words(words):\n",
    "    words = word_tokenize(words)\n",
    "    first_tag = morph.parse(words[1].lower())[0]\n",
    "    second_tag = morph.parse(words[0].lower())[0]\n",
    "    \n",
    "    if first_tag.tag.POS == 'NOUN' and (second_tag.tag.POS == 'ADJF' or second_tag.tag.POS == 'ADJS'):\n",
    "        cut_adj = stemmer.stemWord(words[1].lower())\n",
    "        cut_noun = stemmer.stemWord(words[0].lower())\n",
    "\n",
    "        normal_noun_tag = morph.parse(first_tag.normal_form)[0].tag\n",
    "        normal_noun = first_tag.normal_form\n",
    "        normal_adj = morph.parse(second_tag.normal_form)[0]\n",
    "\n",
    "        if normal_noun_tag.gender is not None:\n",
    "            if normal_noun_tag.gender == 'masc' and normal_adj.inflect({'masc'}):\n",
    "                normal_adj = normal_adj.inflect({'masc'}).word\n",
    "            elif normal_noun_tag.gender == 'femn' and normal_adj.inflect({'femn'}):\n",
    "                normal_adj = normal_adj.inflect({'femn'}).word\n",
    "            elif normal_noun_tag.gender == 'neut' and normal_adj.inflect({'neut'}):\n",
    "                normal_adj = normal_adj.inflect({'neut'}).word\n",
    "            else:\n",
    "                normal_adj = morph.parse(second_tag.normal_form)[0].word\n",
    "        else:\n",
    "            normal_adj = morph.parse(second_tag.normal_form)[0].word\n",
    "\n",
    "        return cut_noun +\" \"+ cut_adj\n",
    "    return words[1].lower() + \" \" + words[0].lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"01/09/2022\": [[\"справедливый казахстан\", 0.03213905339601955], [\"уязвимая категория\", 0.012052145023507332], [\"синтетический наркотик\", 0.012052145023507332], [\"январское событие\", 0.009488098732903343], [\"политическую модернизация\", 0.008131485206349601], [\"равную возможность\", 0.008131485206349601], [\"общенациональный референдум\", 0.008034763349004888], [\"решающая роль\", 0.008034763349004888], [\"успешная нация\", 0.008034763349004888], [\"школьная форма\", 0.008034763349004888]]}\n",
      "{'01/09/2022': [['справедливый казахстан', 0.03213905339601955], ['уязвимая категория', 0.012052145023507332], ['синтетический наркотик', 0.012052145023507332], ['январское событие', 0.009488098732903343], ['политическую модернизация', 0.008131485206349601], ['равную возможность', 0.008131485206349601], ['общенациональный референдум', 0.008034763349004888], ['решающая роль', 0.008034763349004888], ['успешная нация', 0.008034763349004888], ['школьная форма', 0.008034763349004888]]}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def ask_gpt3(addresses):\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "    res = {}\n",
    "    for year, props in addresses.items():\n",
    "\n",
    "        text = '''\n",
    "            Here are the top 25 phrases of each year I parsed from news website. \n",
    "            You must choose top 10 ones that are valuable in from point of economic, \n",
    "            culture, medicine, science, tech, sport, life, show, accidents or crime.\n",
    "\n",
    "            For example, \"мой взгляд\", \"всю сфера\" or \"такой ситуация\" are not so valuable \n",
    "            in terms of any of those topics, while \"синтетический наркотик\", \"январское событие\" are valuable\n",
    "\n",
    "            The data is in following format:\n",
    "            \"dd/mm/yyyy\": [\n",
    "              [\"word\", score],\n",
    "              [\"word\", score],\n",
    "            ]\n",
    "            you must return data in the same format as it comes, and no other word! \n",
    "            This is crucial so that I can parse result into python dict.\n",
    "            The score and the word should be correctly matched. Use double quotes, not single quotes\n",
    "\n",
    "            Data:\n",
    "        ''' + str({year:props})\n",
    "\n",
    "        message = [{\"role\": \"system\", \"content\": text}]\n",
    "\n",
    "        chat = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=message\n",
    "        )\n",
    "\n",
    "        reply = chat.choices[0].message.content\n",
    "        dict_reply = json.loads(reply)\n",
    "        for k,v in dict_reply.items():\n",
    "            res[k] = v\n",
    "\n",
    "    return res\n",
    "\n",
    "data = {\n",
    "  \"01/09/2022\": [\n",
    "    [\"справедливый казахстан\", 0.03213905339601955],\n",
    "    [\"уязвимая категория\", 0.012052145023507332],\n",
    "    [\"синтетический наркотик\", 0.012052145023507332],\n",
    "    [\"электоральный цикл\", 0.012052145023507332],\n",
    "    [\"январское событие\", 0.009488098732903343],\n",
    "    [\"политическую модернизация\", 0.008131485206349601],\n",
    "    [\"равную возможность\", 0.008131485206349601],\n",
    "    [\"общенациональный референдум\", 0.008034763349004888],\n",
    "    [\"решающая роль\", 0.008034763349004888],\n",
    "    [\"успешная нация\", 0.008034763349004888],\n",
    "    [\"школьная форма\", 0.008034763349004888],\n",
    "    [\"различная мера\", 0.008034763349004888],\n",
    "    [\"общенациональный интерес\", 0.008034763349004888],\n",
    "    [\"силовому орган\", 0.008034763349004888],\n",
    "    [\"семейно-бытового насилие\", 0.008034763349004888],\n",
    "    [\"уголовно-процессуальный кодекс\", 0.008034763349004888],\n",
    "    [\"один срок\", 0.008034763349004888],\n",
    "    [\"справедливое распределение\", 0.006325399155268895],\n",
    "    [\"инвестиционная привлекательность\", 0.006325399155268895],\n",
    "    [\"базовый фактор\", 0.006325399155268895],\n",
    "    [\"указанная мера\", 0.006325399155268895],\n",
    "    [\"данный шаг\", 0.006325399155268895],\n",
    "    [\"светлое будущее\", 0.006325399155268895],\n",
    "    [\"полноценный институт\", 0.006325399155268895],\n",
    "    [\"массовый беспорядок\", 0.006325399155268895]\n",
    "  ]\n",
    "}\n",
    "print(ask_gpt3(data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "topics = read_json_data(\"../articles/map_data.json\")\n",
    "addresses = read_json_data(\"../addresses/tf_idf.json\")\n",
    "for date, words in addresses.items():\n",
    "    for word_score in words:\n",
    "        word, score = word_score\n",
    "        morphed_word = morph_words(word)\n",
    "\n",
    "        if morphed_word in topics:\n",
    "            word_score.append(topics[morphed_word][1])\n",
    "        else:\n",
    "            max_ratio = [-1,\"\"]\n",
    "            for word, props in topics.items():\n",
    "                simil_ratio = SequenceMatcher(None, morphed_word, word).ratio()\n",
    "                if max_ratio[0] < simil_ratio:\n",
    "                    max_ratio = [simil_ratio, props[1]]\n",
    "\n",
    "            if max_ratio[0] == -1:\n",
    "                print(morphed_word)\n",
    "                word_score.append(\"undefined topic\")\n",
    "            else:\n",
    "                word_score.append(max_ratio[1])\n",
    "\n",
    "save_data_to_json(\"final\", addresses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
